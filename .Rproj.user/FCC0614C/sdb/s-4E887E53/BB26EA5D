{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Exploratory Data Mining via Search Strategies Lab #3\"\nauthor: \"Ross Jacobucci & Kevin J. Grimm\"\nfontsize: 8pt\noutput:\n  beamer_presentation:\n    colortheme: \"wolverine\"\n    fonttheme: \"structurebold\"\n---\n## Outline\n\nThis script will go over Decision Trees and generalizations in R.\n\\\nResources:\n\\\n\nBasic intro to Decision Trees: http://www.statmethods.net/advstats/cart.html\n\\\nFull list of data mining packages in R: http://cran.r-project.org/web/views/MachineLearning.html\n\\\nTwo packages will be used and their caret equivalents:\n\n- **rpart** (tree accomplishes very similar thing):http://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf\n\\\n\\\n- **party**: http://cran.r-project.org/web/packages/party/vignettes/party.pdf\n\\\n\\\nIn **caret**, method = \n\n- \"rpart\" -- tuning = cp (complexity parameter)\n\n- \"rpart2\" -- tuning = maxdepth\n\n- \"rpartCost\" -- tuning = cp and cost\n\n- \"ctree\" -- tuning = mincriterion (p value thresholds)\n\n- \"ctree2\" -- tuning = maxdepth\n\n## Lets load the main packages\n\n```{r,warning=F,message=FALSE}\nlibrary(caret)\nlibrary(rpart)\nlibrary(pROC)\nlibrary(randomForest)\nlibrary(gbm)\nlibrary(ISLR)\nlibrary(party)\nlibrary(MASS) # for boston data\ndata(Boston)\n```\n\n# Regression\n\n## Regression (continous outcome)\n\nUse rpart first with the Boston data\nuse regression first -- predicting median value of homes\n\n```{r}\n#str(Boston)\n\n# lets get a baseline with linear regression\nlm.Boston <- lm(medv ~., data=Boston)\n#summary(lm.Boston)\n```\n\nWe do pretty well with linear regression\nR-squared of .74\n\n\n\n## CART\n\nHow about if we just blindly apply Decision Trees\n\n```{r,fig.height=6}\nrpart.Boston <- rpart(medv ~., data=Boston)\n#summary(rpart.Boston)\nplot(rpart.Boston);text(rpart.Boston)\n```\n\n## CART Continued\n\n```{r}\npred1 <- predict(rpart.Boston)\ncor(pred1,Boston$medv)**2\n```\nDoing really well -- Rsquared = 0.81\\\n\n\n```{r,fig.height=6}\n# this can be hard to interpret, so I like to look at a different output\nrpart.Boston\n```\n\n## Lasso Regression\n\nWhat if we tried regularized (penalized) regression instead?\\\nNote: for glmnet, both the x's and y have to be in separate matrices\\\n-- and all class = numeric\\\n-- don't worry about response, doesn't have to be factor for logistic\\\n----- just specify \"binomial\"\\\n\n```{r,message=FALSE}\ny.B <- Boston$medv\nx.B <- sapply(Boston[,-14],as.numeric)\n\n# alpha =1 for lasso, 0 for ridge\nlibrary(glmnet)\ncv <- cv.glmnet(x.B,y.B,alpha=1)\nlasso.reg <- glmnet(x.B,y.B,alpha=1,family=\"gaussian\",lambda=cv$lambda.min)\n\nlasso.resp <- predict(lasso.reg,newx=x.B)\ncor(y.B,lasso.resp)**2\n\n```\n\nTaking into account cross-validation, we do worse compared to linear regression with no tuning.\n\n## CART Plots\n\nSo the plot for rpart didn'tcome out that well.\\\nGood news, there are better options for plotting.\\\n\\\nhttp://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html\n\\\nLet's load some new packages:\n```{r,message=FALSE}\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nlibrary(partykit)\n```\n\nNote: rattle is package that uses a GUI (think SPSS) for data mining applications\ncheck out book: http://www.amazon.com/Data-Mining-Rattle-Excavating-Knowledge/dp/1441998896\n\nAnyways, lets try some new, prettier plots:\n\n## prp()\n\n```{r,fig.height=5}\n# prp(); from rpart.plot\nprp(rpart.Boston);text(rpart.Boston)\n\n```\n\n## CART Plotting Continued\n\nNote, prp() offers many additional capabilities for tweaking the plot\nFor instance:\n```{r}\n# ?prp\nprp(rpart.Boston,varlen=10,digits=5,fallen.leaves=T)\n```\n\n## CART Plotting Continued\n\n### Probably my favorite, fancyRpartPlot()\n\n```{r,fig.height=5}\n#fancyRpartPlot(); from rattle\nfancyRpartPlot(rpart.Boston)\n```\n\n\n\n## Conditional Inference Trees\nSo what about with conditional inference trees?\n\nWhat if we want a smaller tree? This can be accomplished a number of ways. We can prespecify the maxdepth, the minimum number of people per node, as well as making more restrictive splitting criterion.\n\nExample of prespecifying the depth with ctree()\n```{r}\nctree.Boston <- ctree(medv ~., data=Boston)\n#plot(ctree.Boston) # too big of a tree\npred2 <- predict(ctree.Boston)\ncor(pred2,Boston$medv)**2\n```\nWe do better than rpart, Rsquared = 0.87\n\n## Conditional Inference Trees Continued\n\nBiggest difference between ctree() and rpart() is that ctree() does not demonstrate bias with respect to the number of response options, and supposedly had less of a propensity to overfit than rpart().\n\\\nNote: the models are not optimizing based on Rsquared, most likely MSE\\\n\\\nSo what do we think now? Are we happy with results?\nRemember, decision trees are generally quite robust, so it may not be necessary to check assumptions. -- See Table 10.1 ESL\\\n\\\nBut what about generalizability?\\\n\\\nAlthough not as serious as with SVM for instance, Decision Trees have a propensity to overfit, meaning the tree structure won't generalize well\\\n\\\nSo let's try just creating a simple Training and Test datasets\n\n```{r}\ntrain = sample(dim(Boston)[1], dim(Boston)[1]/2) # half of sample\nBoston.train = Boston[train, ]\nBoston.test = Boston[-train, ]\n```\n\n\n## Linear Regression with CV\n\n```{r}\nlm.train <- lm(medv ~., data=Boston.train)\n\npred.lmTest <- predict(lm.train,Boston.test)\ncor(pred.lmTest,Boston.test$medv)**2\n```\nNote: we are taking our lm object trained on the train dataset, and using these fixed coefficients to predict values on the test dataset.\\\n\\\nIn SEM, this is referred to as a tight replication strategy\nNo difference in using a test dataset -- both Rsq are 0.74\\\n\\\nHow about with rpart?\n\n## CART CV\n\n```{r}\nrpart.train <- rpart(medv ~., data=Boston.train)\n\npred.rpartTest <- predict(rpart.train,Boston.test)\ncor(pred.rpartTest,Boston.test$medv)**2\n\n```\n\nNot as good -- drops from 0.81 to 0.76 -- still better than lm()\\\n\nBut with rpart, it is common to prune trees back. What if we try this, is there less of a drop in $R^{2}$?\n\nNote: rpart automatically does internal CV, varying the complexity paramter (cp). If you use the tree package instead, you will have to use cv.tree()\n\n## CART Pruning\n\nWith plotcp() we are going to choose the error within 1 SE of the lowest cross-validated error. This will be used to prune\n\n```{r}\nplotcp(rpart.train)\n```\n\n## CART Pruning Continued\n\n```{r}\nprintcp(rpart.train)\n```\n\n\n## Rpart Error Explained\n\nrel error -- The error on the whole sample (lower  the better)\n\n\nxerror -- Error on the cross-validation folds\n\n\nxstd -- variability in error across CV folds\n\n## CART Pruning Continued\n\n```{r}\n# rsq.rpart(rpart.train) ## another way to get cp plots\nprune.Bos <- prune(rpart.train,0.053)\n\npred.prune <- predict(prune.Bos,Boston.test)\ncor(pred.prune,Boston.test$medv)\n```\n\nPlot Pruned Tree\n```{r,fig.height=3.5}\n#plot(prune.Bos);text(prune.Bos)\nfancyRpartPlot(prune.Bos)\n\n```\n\n## CTree CV\n\n\n```{r}\nctree.train <- ctree(medv ~., data=Boston.train)\n#plot(ctree.train) # huge tree\npred.ctreeTest <- predict(ctree.train,Boston.test)\ncor(pred.ctreeTest,Boston.test$medv)**2\n\n```\n\nIt is worth noting how much more of an effect there was for using a test dataset with the tree methods as compared to lm(), this is pretty typical, and much more important with more \"flexible\" methods such as random forests, gbm, svm etc...\n\n\n# Classification\n\n## Classification\n\n**Two Biggest Things To Remember:**\\\n1. Make sure functions outcome variable is categorical; as.factor(outcome)\\\n2. Using predict() changes. Variable across packages\\\n\\\n\nAs a baseline, we will use logistic regression.\n```{r}\nlibrary(ISLR)\ndata(Default)\n#head(Default)\nstr(Default)\n```\nMy favorite function in R is str(), as it gives the class of each variable and other summary characteristics. Most important thing to note is that the \"default\" variable is already coded as a factor variable, meaning that R now knows it is categorical, and will change the cost function (thus estimator) accordingly.This is really important because rpart,randomForest and other packages do not automatically detect whether it is a regression or classification problem. If you don't change the outcome variable to its proper class, you could get a suboptimal answer (use the wrong estimator i.e. regression instead of logistic regression)\\\n\n## Logistic Regression\nNow let's do logistic regression\n```{r}\nlr.out <- glm(default~student+balance+income,family=\"binomial\",data=Default)\nsummary(lr.out)\n```\n\nI always find it much harder to figure out how well I am doing with logistic regression. One of the best ways to assess results in my opinion is the use of receiver operating characteristic curves (ROC curves).\\\n\n\n## ROC Curves\n\ngood intro to using ROC:\\\nhttps://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf\n\\\nThese plots are a balanace of sensitivity and specificity. Ideally the curve gets as close as possible to the upper left corner.\\\n\\\nTo get this plot, we need to get our predictions from our logistic model.\n\n```{r,warning=FALSE,message=FALSE}\nglm.probs=predict(lr.out,type=\"response\")\n#glm.pred00=ifelse(glm.probs>0.5,1,0)\n\nrocCurve <- roc(Default$default,glm.probs)\npROC::auc(rocCurve)\npROC::ci(rocCurve)\n```\n\n## Plot ROC Curve\n```{r}\n# quartz()\nplot(rocCurve, legacy.axes = TRUE,print.thres=T,print.auc=T)\n```\n\nFor AUC (area under the curve), values of 0.8 and 0.9 are good (the higher the better)\n\n## Lasso Logistic Regression\n\nHow about lasso logistic regression?\n```{r}\nlibrary(glmnet)\nyy = as.numeric(Default$default)\nxx = sapply(Default[,2:4],as.numeric)\nlasso.out <- cv.glmnet(xx,yy,family=\"binomial\",alpha=1,nfolds=10) #alpha=1 ==  lasso; 0 = \n# find best lambda\nll <- lasso.out$lambda.min\n\nlasso.probs <- predict(lasso.out,newx=xx,s=ll,type=\"response\")\n```\n\nResults from lasso using CV\n```{r,warning=FALSE,message=FALSE}\n\nrocCurve.lasso <- roc(Default$default,lasso.probs)\npROC::auc(rocCurve.lasso)\npROC::ci(rocCurve.lasso)\n```\n\n## Plot ROC\n```{r}\n# quartz()\nplot(rocCurve.lasso, legacy.axes = TRUE,print.thres=T,print.auc=T)\n```\n\n\nAlmost identical results to logistic regression with no penalization.\n\n## Using Decision Trees for Classification\n\nInstead of demonstrating how to use rpart() or ctree(), I prefer to use the train() from caret. This makes it much easier to test out multiple different methods, as well as automatically vary the tuning parameters such as depth, complexity etc..\n\ntrain() for ctree\n```{r,fig.height=5}\ntrain.ctree <- train(as.factor(default)~\n              student+balance+income,data=Default,method=\"ctree\")\nplot(train.ctree)\n```\n\n\n\n## train() for rpart\n```{r,fig.height=4}\ntrain.rpart <- train(as.factor(default)~\n              student+balance+income,data=Default,method=\"rpart\")\nplot(train.rpart)\n```\n\n\n## train() Continued\nIn train() and through trainControl() you can see that it automatically varies different tuning parameters (see caret documentation for the different options for each method), while defaulting to bootstrap estimation to test out each. This is a great way to prevent overfitting.\n\nIn examining both plots, it seems as both methods do comparably well, while also they both have different tuning parameters (X-axis). Based on these plots, I would increase the number of values for the tuning parameters, as the accuracy did not reach a maximum necessarily outside of the tails. (tuneLength = 3 is default)\n\n\n## using a confusion matrix\n\n```{r}\n\ntrain.class <- predict(train.rpart,Default,type=\"raw\")\nconfusionMatrix(train.class,Default$default)\n```\n\n\n## Confusion matrix terminology\n\n\nAccuracy -- TP + TN/ total\n\\\n\\\nSpecificity -- TN/ Actual Negative\n\\\n\\\nSensitivity -- TP/ Actual Positive\n\\\n\\\nhttps://en.wikipedia.org/wiki/Confusion_matrix\n\nClass imbalance: https://www.r-bloggers.com/handling-class-imbalance-with-r-and-caret-an-introduction/\n\n\n## table()\n\nThe typical way of getting a table of classification results:\n\n```{r}\ntable(train.class,Default$default)\n```\n\nI use confusionMatrix() because I am lazy and don't want to calculate all of the other statistics\n\n\n## Changing the cutoff for class assignment\n\nThis uses the optimal cutoff from the pROC plot\n\n```{r}\ntrain.prob <- predict(train.rpart,Default,type=\"prob\")[,2]\ntrain.class2 <- ifelse(train.prob > .031,1,0)\nconfusionMatrix(as.numeric(Default$default)-1,train.class2,positive=\"1\")\n# much better than just using\n#table(train.class2,Default$default)\n```\n\n\n## Another Way to Get Optimal Threshold\n```{r,message=FALSE,fig.height=3.5}\nlibrary(ROCR)\npred <- prediction(train.prob,as.numeric(Default$default)-1)\nperf <- performance(pred,\"tpr\",\"fpr\")\n#str(perf)\nplot(perf)\ncutoffs <- data.frame(cut=perf@alpha.values[[1]], fpr=perf@x.values[[1]], \n                      tpr=perf@y.values[[1]])\n```\n\n\n\n# Boosting and Random Forests\n\n## Boosting and RF Setup\n\nFor this, we will use the **caret** package as an interface to both the **gbm** and **randomForest** packages.\n\\\n\\\nBecause gbm and random forests take much longer to run, we could set up parallelization through caret and other packages.\nhttp://topepo.github.io/caret/parallel.html\n\\\n\\\nIn my experience, unless your dataset is huge, parallelization with random forests tends to take longer than setting up only serial computation.\n\\\n\\\n\nIn caret, randomForest has two implementations, method=\"rf\" and method=\"parRF\" with parRF being the parallel version. The only tuning parameter for both is mtry.\n\\\n\\\nNote, that using the train() will take longer, as it is using different tuning parameters and by default using bootstrap sampling to prevent overfitting. \n\\\n\\\nTo let train() pick the values of mtry, just set tuneLength to however many different values you want it to try. Default is 3.\n\n\n## Run Random Forest\n\n```{r}\n#library(snowfall);#sfInit(parallel=T,cpus=4)\ncont <- trainControl(allowParallel=TRUE,method=\"cv\")\n```\n\n\n```{r}\ntrain.rf1 <- train(medv ~ ., data=Boston.train,method=\"rf\",\n                   trControl=cont,\n                   importance=T,tuneLength=3)\n\ntrain.rf1\n```\n\n## Performance on a holdout\n```{r}\n# see how we do on hold out sample\n# automatically chooses best model for prediction\npred.test1 <- predict(train.rf1,Boston.test)\ncor(pred.test1,Boston.test$medv)**2\n```\n\nBest practice is to only do this with one model. I.e. choose between best random forest and boosting models, take this one model and check performance on test dataset and report.\n\n## Plot Performance\n\n```{r,fig.height=4}\nplot(train.rf1)\n```\n\n## Variable Importance\n\n```{r,fig.height=4}\n#rf <- train.rf1$finalModel\nimp <-varImp(train.rf1)\nplot(imp)\n```\n\n\n\n## Cforest\ncforest is implemented as method=\"cforest\" with the only tuning parameter being mtry\n\n\n```{r}\ntrain.cf1 <- train(medv ~., data=Boston.train,method=\"cforest\")\n#train.cf1\n\n#plot(train.cf1)\n\n#varImp(train.cf1)\n\n# see how we do on hold out sample\npred.test2 <- predict(train.cf1,Boston.test)\ncor(pred.test2,Boston.test$medv)**2\n\n```\n\n## Classification\nFor classification, a couple other things to use paired with train()\n\n```{r}\n# set up data\ndata(Carseats)\n#attach(Carseats)\n#hist(Carseats$Sales)\nHigh=ifelse(Carseats$Sales<=8,\"No\",\"Yes\")\nCarseats=data.frame(Carseats, High)\nCarseats$Sales <- NULL\nCarseats$ShelveLoc <- as.numeric(Carseats$ShelveLoc)\n\n\ntrain2 = sample(dim(Carseats)[1], dim(Carseats)[1]/2) # half of sample\nCarseats.train = Carseats[train2, ]\nCarseats.test = Carseats[-train2, ]\n```\n\n\n\n## Now run random forests\n```{r}\ntrain.rf2 <- train(as.factor(High) ~ ., \n                   data=Carseats.train,method=\"rf\",trControl=cont)\n#train.rf2\nrf.probs=predict(train.rf2,newdata=Carseats.test,type=\"prob\")[,2]\nrocCurve22 <- roc(Carseats.test$High,rf.probs)\n#auc(rocCurve22);#\nrf.class=predict(train.rf2,newdata=Carseats.train)\nconfusionMatrix(Carseats.train$High,rf.class)\n\n```\n\n## Confusion Matrix on Test Data\n```{r}\n\n#auc(rocCurve4);#ci(rocCurve4);#plot(rocCurve4)\nrf.classTest=predict(train.rf2,Carseats.test)\nconfusionMatrix(Carseats.test$High,rf.classTest)\n```\n\n\n\n## cforest for binary \n\n### automatically knows it is binary, unlike randomForest\n\n\n```{r}\ntrain.cf2 <- train(High ~ ., \n            data=Carseats.train,method=\"cforest\",trControl=cont)\n#train.cf2\ncf.probs=predict(train.cf2) \nrocCurve33 <- roc(Carseats.train$High,as.numeric(cf.probs));#auc(rocCurve33)\n#plot(rocCurve33,add=T,col=3)\ncf.class=predict(train.cf2$finalModel)\nconfusionMatrix(Carseats.train$High,cf.class) # no errors\n\n```\n\n\n## CForest on Test Data\n```{r}\ncf.classTest=predict(train.cf2,newdata=Carseats.test)\ncf.probs2=predict(train.cf2,newdata=Carseats.test,type=\"prob\")[,2]\nrocCurve333 <- roc(Carseats.test$High,as.numeric(cf.probs2));#auc(rocCurve33)\n#plot(rocCurve333,add=T,col=3)\n```\n\n## CForest Test Confusion Matrix\n```{r}\nconfusionMatrix(Carseats.test$High,cf.classTest)\n\n```\n\n## Boosting\npackages: \"gbm\" and \"ada\" -- both can be accessed through caret\n\n\nExample tuning parameters for \"gbm: http://topepo.github.io/caret/training.html\n\n\nFor this, I am going to use method=\"ada\" which is one of the forms of boosting. Currently, problem with method=\"gbm\"\n\nBasic set up to compare results to RF:\n```{r,message=FALSE,warning=FALSE}\n\n#modelLookup(\"ada\")\ntrain.gbm <- train(as.factor(High) ~ ., verbose=F,\n                   data=Carseats.train,method=\"gbm\",trControl=cont)\n#train.gbm\n\ngbm.probs=predict(train.gbm,newdata=Carseats.test,type=\"prob\")[,2]\nrocCurve3 <- roc(Carseats.test$High,gbm.probs)\n#auc(rocCurve3)\n#ci(rocCurve3)\n```\n\n## Compare ROC Curves\n\n\n```{r,fig.height=4.5,message=FALSE,warning=FALSE,comment=FALSE,results=\"hide\"}\nplot(rocCurve22,col=c(1)) # color black is rf\nplot(rocCurve333,add=T,col=c(2)) # color red is cforest\nplot(rocCurve3,add=T,col=c(3)) # color green is gbm\n\n```",
    "created" : 1483484995546.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "997215259",
    "id" : "BB26EA5D",
    "lastKnownWriteTime" : 1481555435,
    "last_content_update" : 1481555435,
    "path" : "~/GitHub/SearchWkshp_labs16/lab3.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}